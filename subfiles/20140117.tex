\documentclass[../main.tex]{subfiles}
\begin{document}

\subsection{Pairwise Swap}
\begin{question}
Consider p processes (p is even), each process has 1 data element. The even processes s swap their element with their neighbour s+1 (= pair-wise swap).
\begin{enumerate}
	\item Give a BSP algorithm.
	\item What is the $h$-relation?
	\item Suppose (non-BSP) 2-sided send and receive. When send transmits, hold execution until remote process receives. Then execute the communication and both processes continue. This is blocking pair-wise communication (as featured in MPI). Give the algorithm.
	\item Executing a send-receive communication has start-up time $s$, and data is transfered with bandwidth $b$. Is $s < l$? Will the cost be higher than the BSP algorithm from 1.1?
\end{enumerate}
\end{question}
\begin{solution} The solution to the subquestions are given below.
\begin{enumerate}
	\item The BSP program for one-to-all communication is given below.
\begin{lstlisting}
void pairwiseSwap(){
	bsp_begin(P);
	bsp_push_reg(element);
	int s = bsp_pid();
	if(s % 2 == 0){
		bsp_put(s+1, data, element, 0, sizeof(data));
	}else if{
		bsp_put(s-1, data, element, 0, sizeof(data));
	}
	bsp_sync();
	bsp_end();
}
\end{lstlisting}
	\item The $h$-relation is given by $h = max\{h_s,h_r\}$. In the previous case, a processor sends one element and receives one. Therefore, $h = 1$. The total cost is $T = g + l$.
	\item The non-BSP program for one-to-all communication is given below. We assume primitives \texttt{send(message, dest)} and \texttt{receive(message, sender)}. The data is not buffered.
\begin{lstlisting}
void pairwiseSwap(){
	init_comm(s,P);

	if(s % 2 == 0){
		send(s+1, data);
		receive(data, sendproc);
		if(sendproc != s + 1)
			ERROR
	}else if{
		receive(data, sendproc);
		send(s-1, data;
		if(sendproc != s - 1)
			ERROR
	}
}
\end{lstlisting}
\item The message-passing cost is modeled as $T(n) = s + bn$. Because each processor sends one element and receives another, the total cost is given by $T = 2(s+b)$. Assuming the same cost per word ($g=b$), the BSP algorithm is faster unless $2(s+g) < g + l$ or $2s < l - g$.
\end{enumerate}
\end{solution}

\subsection{SpMV}
\begin{question}
$A$ is a sparse $m$ by $n$ matrix, $x$ and $y$ are vectors.
\begin{enumerate}
	\item What is the sequential cost of $y=Ax$?
	\item Assume $A$ is distributed row-wise in a 1D fashion. Which phase, if any, of the classic SpMV phases (fan-out, SpMV, fan-in) will disappear? What will be the new total cost? What will be the parallel overhead?
	\item $H=(V,N)$ is a hypergraph of A. What hypergraph model would you use to distribute A using the previous 1D distribution type? Give definitions of $V$ and $N$ in that model.
	\item Assume SpMV algorithm 4.5 as in the book. Use your definition of $H$ from the previous question to write a cost function that measures the number of gets and puts.
\end{enumerate}
\end{question}
\begin{solution}
\end{solution}

\subsection{Odd-Even Transposition}
\begin{question}
Array $x$ has $n$ elements with $n$ a multiple of $p$. In odd-even transposition sort, the fundamental operation is compare-exchange (when each process has 1 element, $n = p$) or compare-split (each process has $n/p$ elements). Give for both cases:
\begin{enumerate}
	\item A BSP algorithm.
	\item Parallel execution time.
	\item Parallel efficiency.
	\item For n/p elements: if n increases (with a fixed p), what will happen to the parallel efficiency?
\end{enumerate}



\end{question}
\begin{solution}
\end{solution}

\subsection{Shared Memory}
\begin{question}
We have p threads running the same SPMD program on a shared memory computer. Will the following algorithm work? Describe any possible problems. Supply a working parallel algorithm.
\begin{lstlisting}[caption={SPMD program on a shared memory computer},label=lst:spmd]
double a;
double x[1000];
double y[1000];

void spmd(){
	for (i=0; i<1000; i+=p)
		a = a + x[i]y[i];
}
\end{lstlisting}

\end{question}
\begin{solution}
There are two common pitfalls with non-BSP shared memory implementations: \textbf{data races}, \textbf{false sharing} and \textbf{inefficient cache use}. The given problem is an illustration of the first. A data race occurs when multiple processors access the same memory location concurrently and at least one of them is writing. This leads to inconsistencies and the value most likely to be incorrect.
\\\\
We briefly discuss the other two pitfalls while giving the correct implementation. One solution  is storing the partial sums separately. Using an array of $p$ elements however would lead to \textbf{false sharing}. On every iteration, the array would be marked dirty in the cache, even though the value change at index i (at $p(i)$) is logically independent of the chance of value at index j (at $p(j)$). The last pitfall occurs when we choose to extend the array so that each cache line is only accessed by a single processor. This works, but leads to inefficient cache use because all processors would be accessing all cache lines. The correct solution is given below.

\begin{lstlisting}[caption={SPMD program on a shared memory computer},label=lst:spmd]
double a[8p];
double x[1000];
double y[1000];
int n = 1000;
void spmd(){
	for (i=s*ceil(n/p); i<(s+1)*ceil(n/p)); i++)
		a[8s] += x[i]y[i];
}
\end{lstlisting}


\end{solution}

\end{document}