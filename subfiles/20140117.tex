\documentclass[../main.tex]{subfiles}
\begin{document}

\subsection{Pairwise Swap}
\begin{question}
Consider p processes (p is even), each process has 1 data element. The even processes s swap their element with their neighbour s+1 (= pair-wise swap).
\begin{enumerate}
	\item Give a BSP algorithm.
	\item What is the $h$-relation?
	\item Suppose (non-BSP) 2-sided send and receive. When send transmits, hold execution until remote process receives. Then execute the communication and both processes continue. This is blocking pair-wise communication (as featured in MPI). Give the algorithm.
	\item Executing a send-receive communication has start-up time $s$, and data is transfered with bandwidth $b$. Is $s < l$? Will the cost be higher than the BSP algorithm from 1.1?
\end{enumerate}
\end{question}
\begin{solution} The solution to the subquestions are given below.
\begin{enumerate}
	\item The BSP program for one-to-all communication is given below.
\begin{lstlisting}
void pairwiseSwap(){
	bsp_begin(P);
	bsp_push_reg(element);
	int s = bsp_pid();
	if(s % 2 == 0){
		bsp_put(s+1, data, element, 0, sizeof(data));
	}else if{
		bsp_put(s-1, data, element, 0, sizeof(data));
	}
	bsp_sync();
	bsp_end();
}
\end{lstlisting}
	\item The $h$-relation is given by $h = max\{h_s,h_r\}$. In the previous case, a processor sends one element and receives one. Therefore, $h = 1$. The total cost is $T = g + l$.
	\item The non-BSP program for one-to-all communication is given below. We assume primitives \texttt{send(message, dest)} and \texttt{receive(message, sender)}. The data is not buffered.
\begin{lstlisting}
void pairwiseSwap(){
	init_comm(s,P);

	if(s % 2 == 0){
		send(s+1, data);
		receive(data, sendproc);
		if(sendproc != s + 1)
			ERROR
	}else if{
		receive(data, sendproc);
		send(s-1, data;
		if(sendproc != s - 1)
			ERROR
	}
}
\end{lstlisting}
\item The message-passing cost is modeled as $T(n) = s + bn$. Because each processor sends one element and receives another, the total cost is given by $T = 2(s+b)$. Assuming the same cost per word ($g=b$), the BSP algorithm is faster unless $2(s+g) < g + l$ or $2s < l - g$.
\end{enumerate}
\end{solution}

\subsection{SpMV}
\begin{question}
$A$ is a sparse $M \times N$ matrix, $x$ and $y$ are vectors.
\begin{enumerate}
	\item What is the sequential cost of $y=Ax$?
	\item Assume $A$ is distributed row-wise in a 1D fashion. Which phase, if any, of the classic SpMV phases (fan-out, SpMV, fan-in) will disappear? What will be the new total cost? What will be the parallel overhead?
	\item $H=(V,N)$ is a hypergraph of A. What hypergraph model would you use to distribute A using the previous 1D distribution type? Give definitions of $V$ and $N$ in that model.
	\item Assume SpMV algorithm 4.5 as in the book. Use your definition of $H$ from the previous question to write a cost function that measures the number of gets and puts.
\end{enumerate}
\end{question}
\begin{solution} The solution to the subquestions are given below.
\begin{enumerate}
	\item The sequential cost of $y=Ax$ is $T_{seq} = 2cm$ flops \cite[p.~166]{bisseling04} where $c$ is the fixed number of nonzeroes in a row of A and $m$ the amount of rows. This holds because the used data-structures make it possible to only take multiplications with a nonzero into account.
	\item The fan-in step will disappear. Because if a processor holds an entire row it can calculate the partial sum $y_i$ of that row easiliy without relying on any other partial computations \cite[p.~176]{bisseling04}.  The total (worst case) cost of the unchanged algorithm is
	\begin{equation}
		T_{MV} \leq \frac{2cn}{p} + n + 2 \Big( 1 - \frac{1}{p} \Big) ng + 4l
	\end{equation}
			where the fan-in superstep costs $T_{(2)} = (1-\frac{1}{p})ng + l$ and the last superstep $T_{(3)} = n + l $ \cite[p.~178]{bisseling04}. Because there is only one partial sum per row with the given distribution $T_{(3)}' = \frac{n}{p} + l $ and removing $T_{(3)}$ entirely gives us
			\begin{equation}
				T_{MV}' \leq \frac{2cn}{p} + \frac{n}{p} + \Big(1 - \frac{1}{p} \Big) ng + 3l
			\end{equation}
			The parallel overhead overhead is defined as the difference between the normalized cost and the ideal 1. So the overhead here is \cite[p.~141]{bisseling04}:
			\begin{equation}
				O = 1 - \frac{T_{MV}'}{T_{seq}/p} \leq 1 - \frac{\frac{2cn}{p} + \frac{n}{p} + \Big(1 - \frac{1}{p} \Big) ng + 3l}{2cm/p}
			\end{equation}
	\item ``A partitioning of a column-net model of A corresponds to a partitioning of the rows of A (a 1D row-wise distribution)'' \cite{slides6}. The colum-net model $H=(V,N)$ is defined as following: Let $I = \{0,1,\ldots,m-1\}$ and $J = \{0,1,\ldots,n-1\}$ then $I$ represents the rows and thus $V = I$ and $\forall i \in I$ we define a net $n_i \in N$ with
	$$ n_i = \{j \in J ~|~ a_{ij} \neq 0\} $$
	If you now partition $H$ over the available processors we have the hypergraph distribution model. The hypergraph of the example $5 \times 4$ matrix below can be found in \autoref{fig:hypergraph}.

		\begin{equation}
			A_{5\times4} = \begin{bmatrix}
			0 & 3 & 0 & 0 \\
			0 & 0 & 1 & 4 \\
			0 & 0 & 0 & 0 \\
			0 & 5 & 0 & 0 \\
			9 & 0 & 2 & 0
			\end{bmatrix}
		\end{equation}

		\begin{figure}[H]
		\begin{tikzpicture}
		\node[vertex,label=above:\(v_0\)] (v0) {};
		\node[vertex,below of=v0,label=above:\(v_1\)] (v1) {};
		\node[vertex,right of=v0,label=above:\(v_2\)] (v2) {};
		\node[vertex,right of=v1,label=above:\(v_3\)] (v3) {};
		\node[vertex,right of=v2,label=above:\(v_4\)] (v4) {};
		\begin{pgfonlayer}{background}
			\draw[edge,color=yellow] (v0) -- (v2);
			\draw[edge,opacity=1,color=green] (v2) -- (v3);
			\draw[edge,color=red] (v1) -- (v1);
			\draw[edge,line width=60pt ,color=blue] (v1) -- (v1);
		\end{pgfonlayer}

		\node[elabel,color=red,label=right:\(n_0\)]  (n0) at (-3,0) {};
		\node[elabel,below of=n0,color=green,label=right:\(n_1\)]  (n1) {};
		\node[elabel,below of=n1,color=orange,label=right:\(n_2\)]  (n2) {};
		\node[elabel,below of=n2,color=blue,label=right:\(n_3\)]  (n3) {};
		\node[elabel,below of=n3,color=yellow,label=right:\(n_3\)]  (n4) {};
		\end{tikzpicture}
		\centering
		\caption{The hypergraph of the $A_{5\times4}$ matrix.}
		\label{fig:hypergraph}
		\end{figure}
	\item Since we are working with a column-net model coarsening means combining similar rows. As example partition we can partition $H$ for 3 processors like this: $\{\{0,1\},\{2,3\},\{4\}\}$.
\end{enumerate}
\end{solution}

\subsection{Odd-Even Transposition}
\begin{question}
Array $x$ has $n$ elements with $n$ a multiple of $p$. In odd-even transposition sort, the fundamental operation is compare-exchange (when each process has 1 element, $n = p$) or compare-split (each process has $n/p$ elements). Give for both cases:
\begin{enumerate}
	\item A BSP algorithm.
	\item Parallel execution time.
	\item Parallel efficiency.
	\item For n/p elements: if n increases (with a fixed p), what will happen to the parallel efficiency?
\end{enumerate}

\end{question}
\begin{solution} The solution to the subquestions are given below.
\begin{enumerate}
	\item A BSP algorithm for the odd-even transposition sort can be found in \autoref{lst:oddeven}.
		\lstinputlisting[caption="BSP odd-even transposition sort.",label=lst:oddeven]{code/oddevenbsp.c}
	\item Parallel execution time.
	\item Parallel efficiency.
	\item For n/p elements: if n increases (with a fixed p), what will happen to the parallel efficiency?
\end{enumerate}
\end{solution}

\subsection{Shared Memory}
\begin{question}
We have p threads running the same SPMD program on a shared memory computer. Will the following algorithm work? Describe any possible problems. Supply a working parallel algorithm.
\begin{lstlisting}[caption={SPMD program on a shared memory computer},label=lst:spmd]
double a;
double x[1000];
double y[1000];

void spmd(){
	for (i=0; i<1000; i+=p)
		a = a + x[i]y[i];
}
\end{lstlisting}

\end{question}
\begin{solution}
There are two common pitfalls with non-BSP shared memory implementations: \textbf{data races}, \textbf{false sharing} and \textbf{inefficient cache use}. The given problem is an illustration of the first. A data race occurs when multiple processors access the same memory location concurrently and at least one of them is writing. This leads to inconsistencies and the value most likely to be incorrect.
\\\\
We briefly discuss the other two pitfalls while giving the correct implementation. One solution  is storing the partial sums separately. Using an array of $p$ elements however would lead to \textbf{false sharing}. On every iteration, the array would be marked dirty in the cache, even though the value change at index i (at $p(i)$) is logically independent of the chance of value at index j (at $p(j)$). The last pitfall occurs when we choose to extend the array so that each cache line is only accessed by a single processor. This works, but leads to inefficient cache use because all processors would be accessing all cache lines. The correct solution is given below.

\begin{lstlisting}[caption={SPMD program on a shared memory computer},label=lst:solspmd]
double a[8p];
double x[1000];
double y[1000];
int n = 1000;
void spmd(){
	for (i=s*ceil(n/p); i<(s+1)*ceil(n/p)); i++)
		a[8s] += x[i]y[i];
}
\end{lstlisting}


\end{solution}

\end{document}
